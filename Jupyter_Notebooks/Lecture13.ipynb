{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture 13: Regression Models\n",
        "\n",
        "We want to develop techniques which allow us to objectively select a good model for fitting the data."
      ],
      "metadata": {
        "id": "2wJDsZFRDCVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWweztwhC2S_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "plt.rcParams['xtick.labelsize']=16      # change the tick label size for x axis\n",
        "plt.rcParams['ytick.labelsize']=16      # change the tick label size for x axis\n",
        "plt.rcParams['axes.linewidth']=1        # change the line width of the axis\n",
        "plt.rcParams['xtick.major.width'] = 3   # change the tick line width of x axis\n",
        "plt.rcParams['ytick.major.width'] = 3   # change the tick line width of y axis\n",
        "rc('text', usetex=False)                # disable LaTeX rendering in plots\n",
        "rc('font',**{'family':'DejaVu Sans'})   # set the font of the plot to be DejaVu Sans"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deciding what set of parameters to optimize over will play a foundational role in extracting interpretable results and meaningful models from data.\n",
        "\n",
        "In the previous few lectures, we explored the interplay of optimizing over $l_2$ and $l_1$ norms.\n",
        "\n",
        "### 1. Generate some synthetic data for a parabola\n",
        "\n",
        "Now, to illustrate further the variety of possible outcomes, we consider the following simple example of synthetic data:\n",
        "\n",
        "$$ f(x) = x^2 + \\mathcal{N}(0, \\sigma) $$\n",
        "\n",
        "Here, $f(x)$ represents noisy measurements of a parabola, and $\\mathcal{N}(0, \\sigma)$ is a normally distributed random variable with mean zero and standard deviation $\\sigma$."
      ],
      "metadata": {
        "id": "BI5TxX3JInsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 4, 100)\n",
        "mu = 0.0\n",
        "sigma = 0.1\n",
        "M = 20\n",
        "f = np.zeros((100, M))\n",
        "for i in range(M):\n",
        "  f[:,i] = x**2 + np.random.normal(mu, sigma, *x.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 4), dpi = 80)\n",
        "plt.plot(x, f)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JG-_5ZplDGlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we show a plot of 20 different measurements with 100 datapoints each for $f(x)$ with a standard deviation of $0.1$. Based on the plot, you can see there is very little deviation.\n",
        "\n",
        "Now, let's try to fit a single measurements to a curve with the shape of $x^p$.  The fit would be trivial if we know that $p=2$, but here, we do not know this information *a priori*.\n",
        "\n",
        "### 2. Perform Regression for different power of $x$\n",
        "\n",
        "Let's try to do regression on the data on four different measurements independently, fitting to $x^p$ where $p\\in[0, 19]$.\n",
        "\n",
        "(See white board for regression construct)."
      ],
      "metadata": {
        "id": "LqxhPwY6M7cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### build matrix A to perform regression\n",
        "A = np.zeros((len(x), M))\n",
        "for j in range(M):\n",
        "  A[:,j] = x**j\n",
        "\n",
        "fig, axs = plt.subplots(2,2)\n",
        "axs = axs.reshape(-1)\n",
        "fig.set_size_inches(10, 7)\n",
        "\n",
        "for j in range(4):\n",
        "  f = x**2 + np.random.normal(mu, sigma, *x.shape)\n",
        "  A_inv = np.linalg.pinv(A) # Least-square fit\n",
        "  x_apprx = A_inv @ f\n",
        "  f_apprx = A @ x_apprx\n",
        "  E2 = np.linalg.norm(f_apprx-f, ord = 2) / np.linalg.norm(f,ord=2)\n",
        "  axs[j].bar(range(len(x_apprx)), x_apprx)\n",
        "  axs[j].set_title(\"L2 Norm Relative Error is: \"+str(\"%.4f\" % E2))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wzfg_703KLPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how our model compares to the original data."
      ],
      "metadata": {
        "id": "wO4ablN60Ulq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2,2)\n",
        "axs = axs.reshape(-1)\n",
        "fig.set_size_inches(10, 7)\n",
        "\n",
        "for j in range(4):\n",
        "  f = x**2 + np.random.normal(mu, sigma, *x.shape)\n",
        "  A_inv = np.linalg.pinv(A) # Least-square fit\n",
        "  x_apprx = A_inv @ f\n",
        "  f_apprx = A @ x_apprx\n",
        "  axs[j].plot(x, f, label = \"raw data\")\n",
        "  axs[j].plot(x, f_apprx, label = \"fit data\")\n",
        "  axs[j].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5NbEPtJPWfLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Now let's try different kinds of regression procedures\n",
        "\n",
        "Different regression procedures:\n",
        "1. **least-squares regression** (`pinv`) (what we have been doing so far)\n",
        "\n",
        "  Moore-Penrose pseudo-inverse (`pinv`) set $\\lambda_1 = \\lambda_2 = 0$\n",
        "2. **the backslash operator** (` \\ `)\n",
        "\n",
        "  The backslash command (` \\ `) for over-determined systems solves the linear system via a $QR$ decomposition\n",
        "3. **least absolute shrinkage and selection operator** LASSO (`lasso`)\n",
        "\n",
        "  The LASSO (`lasso`) set $\\lambda_1 > 0$ and $\\lambda_2 = 0$.\n",
        "4. **robust fit** (`robustfit`)\n",
        "\n",
        "  Robust fit does weighted least-squares fitting. It allows one to leverage robust statistics methods and penalize according to the Huber norm so as to promote outlier rejection\n",
        "5. **ridge regression** (`ridge`)\n",
        "\n",
        "  Ridge regression (ridge) solves (4.40) with $\\lambda_1 = 0$ and $\\lambda_2 > 0$.\n",
        "\n",
        "Linear Models documentation for `sckit-learn`: https://scikit-learn.org/stable/modules/linear_model.html"
      ],
      "metadata": {
        "id": "4cJfMpiq1j3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "## Different regressions\n",
        "lam = 0.1\n",
        "E1 = np.zeros(100)\n",
        "E2 = np.zeros(100)\n",
        "E3 = np.zeros(100)\n",
        "E4 = np.zeros(100)\n",
        "E5 = np.zeros(100)\n",
        "E6 = np.zeros(100)\n",
        "\n",
        "X1 = np.zeros((M,100))\n",
        "X2 = np.zeros((M,100))\n",
        "X3 = np.zeros((M,100))\n",
        "X4 = np.zeros((M,100))\n",
        "X5 = np.zeros((M,100))\n",
        "X6 = np.zeros((M,100))\n",
        "\n",
        "for j in range(100):\n",
        "  f = x**2 + np.random.normal(mu, sigma, *x.shape)\n",
        "\n",
        "  ### least-square regression\n",
        "  a1 = np.linalg.pinv(A)\n",
        "  x1 = a1 @ f\n",
        "  f1 = A @ x1\n",
        "  E1[j] = np.linalg.norm(f-f1, ord=2)/np.linalg.norm(f, ord=2)\n",
        "\n",
        "  ### backslash command (QR decomposition)\n",
        "  x2 = np.linalg.lstsq(A, f, rcond = None)[0]\n",
        "  f2 = A @ x2\n",
        "  E2[j] = np.linalg.norm(f-f2, ord=2)/np.linalg.norm(f, ord=2)\n",
        "\n",
        "  ### LASSO\n",
        "  reg3 = linear_model.Lasso(alpha = lam)\n",
        "  reg3.fit(A, f)\n",
        "  x3 = reg3.coef_\n",
        "  f3 = A @ x3\n",
        "  E3[j] = np.linalg.norm(f-f3, ord=2)/np.linalg.norm(f, ord=2)\n",
        "\n",
        "  ### Combining l1 and l2 penalization\n",
        "  reg4 = linear_model.ElasticNet(alpha = 0.8, l1_ratio=lam)\n",
        "  reg4.fit(A, f)\n",
        "  x4 = reg4.coef_\n",
        "  f4 = A @ x4\n",
        "  E4[j] = np.linalg.norm(f-f4, ord=2)/np.linalg.norm(f, ord=2)\n",
        "\n",
        "  ### huber/robustfit\n",
        "  # matlab's robustfit() does not have an exact sklearn analogue\n",
        "  huber = linear_model.HuberRegressor()\n",
        "  huber.fit(A, f)\n",
        "  x5 = huber.coef_\n",
        "  f5 = A @ x5\n",
        "  E5[j] = np.linalg.norm(f-f5, ord=2)/np.linalg.norm(f, ord=2)\n",
        "\n",
        "  ridge = linear_model.Ridge(alpha=1.0)\n",
        "  ridge.fit(A, f)\n",
        "  x6 = ridge.coef_\n",
        "  f6 = A @ x6\n",
        "  E6[j] = np.linalg.norm(f-f6, ord=2)/np.linalg.norm(f, ord=2)\n",
        "\n",
        "\n",
        "  X1[:,j] = x1\n",
        "  X2[:,j] = x2\n",
        "  X3[:,j] = x3\n",
        "  X4[:,j] = x4\n",
        "  X5[:,j] = x5\n",
        "  X6[:,j] = x6\n",
        "\n",
        "Err = np.column_stack((E1,E2,E3,E4,E5,E6))"
      ],
      "metadata": {
        "id": "i8imenjo0oNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplot explanation: https://vita.had.co.nz/papers/boxplots.pdf"
      ],
      "metadata": {
        "id": "B6lmkUQruUYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = [12, 18]\n",
        "fig,axs = plt.subplots(3,2)\n",
        "axs = axs.reshape(-1)\n",
        "\n",
        "axs[0].boxplot(X1.T, labels = range(M))\n",
        "axs[0].set_title('pinv', fontsize = 16)\n",
        "axs[1].boxplot(X2.T, labels = range(M))\n",
        "axs[1].set_title('lstsq', fontsize = 16)\n",
        "axs[2].boxplot(X3.T, labels = range(M))\n",
        "axs[2].set_title('LASSO', fontsize = 16)\n",
        "axs[3].boxplot(X4.T, labels = range(M))\n",
        "axs[3].set_title('ElasticNet', fontsize = 16)\n",
        "axs[4].boxplot(X5.T, labels = range(M))\n",
        "axs[4].set_title('huber', fontsize = 16)\n",
        "axs[5].boxplot(X6.T, labels = range(M))\n",
        "axs[5].set_title('ridge', fontsize = 16)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "\n",
        "plt.figure()\n",
        "plt.boxplot(Err, labels = ['pinv', 'lstsq', 'LASSO', 'ElasticNet', 'huber', 'ridge'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oX6PCSsEsPZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Use Pseudo-Inverse as an example to see how error changes as a function of number of powers of $x$ in the model"
      ],
      "metadata": {
        "id": "cKbPiiUF5BR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M = 10\n",
        "En = np.zeros((100,M))\n",
        "A = np.zeros((len(x),M))\n",
        "f = x**2\n",
        "for jj in range(M):\n",
        "  for j in range(jj):\n",
        "    A[:,j] = x**j\n",
        "  for j in range(100):\n",
        "    fn = x**2 + np.random.normal(mu, sigma, *x.shape)\n",
        "    A_inv = np.linalg.pinv(A)\n",
        "    X_apprx = A_inv @ fn\n",
        "    f_apprx = A @ X_apprx\n",
        "    En[j,jj] = np.linalg.norm(f-f_apprx, ord=2)/np.linalg.norm(f, ord=2)\n",
        "\n",
        "plt.boxplot(En[:, :], labels = range(0, M))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cTbAzVP-tC73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. In-class exercise\n",
        "\n",
        "Compute the error as a function of power of x for least-square solver and LASSO"
      ],
      "metadata": {
        "id": "y8FmOQEd5Lup"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hrRIAq2M5kKf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}